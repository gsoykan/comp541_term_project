{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20-12-2020 Most Extensive Experiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models used in saved Neural Network\n",
    "using Knet: Knet, AutoGrad, dir, Data, minibatch, Param, @diff, value, params, grad, progress, progress!, KnetArray\n",
    "using IterTools\n",
    "using Base.Iterators\n",
    "using Knet: Ops20.zeroone\n",
    "using Printf, Random, Test, Statistics\n",
    "using Plots; default(fmt=:png)\n",
    "include(\"../utils.jl\")\n",
    "\n",
    "# Define Linear Model\n",
    "struct Linear; w; b; end\n",
    "Linear(i::Int,o::Int) = Linear(param(o, i), param0(o))\n",
    "(m::Linear)(x) = m.w * x .+ m.b\n",
    "(m::Linear)(x, y) = Knet.nll(m(x), y)\n",
    "(m::Linear)(data::Knet.Data) = mean(m(x, y) for (x, y) in data)\n",
    "\n",
    "# MLP Layer - f: can be identity, relu etc,  - pdrop: dropout rate\n",
    "struct MLPLayer; w; b; f; pdrop; end\n",
    "MLPLayer(i::Int,o::Int,f=relu; pdrop=0) = MLPLayer(param(o, i), param0(o), f, pdrop)\n",
    "(l::MLPLayer)(x) = l.f.(l.w * dropout(x, l.pdrop) .+ l.b)\n",
    "\n",
    "# Define convolutional layer:\n",
    "struct Conv2; w; b; f; is_pool_enabled::Bool; end\n",
    "Conv2(w1,w2,nx,ny, f=relu; is_pool_enabled=true) = Conv2(param(w1, w2, nx, ny), param0(1, 1, ny, 1), f, is_pool_enabled)\n",
    "function (c::Conv2)(x) \n",
    "    if c.is_pool_enabled\n",
    "        pool(c.f.(conv4(c.w, x) .+ c.b))\n",
    "    else\n",
    "        c.f.(conv4(c.w, x) .+ c.b)\n",
    "    end\n",
    "end\n",
    "\n",
    "# Define dense layer:\n",
    "struct Dense; w; b; f; p; end\n",
    "Dense(i::Int,o::Int,f=relu;pdrop=0) = Dense(param(o, i), param0(o), f, pdrop)\n",
    "Dense(w, b; f=identity, pdrop=0) = Dense(param(w; atype=Knet.atype()), param(b; atype=Knet.atype()), f, pdrop)\n",
    "(d::Dense)(x) = d.f.(d.w * mat(dropout(x, d.p)) .+ d.b) \n",
    "\n",
    "# Define a chain of layers and a loss function:\n",
    "struct DeeperCutOption\n",
    "    connect_res3_to_res5::Bool\n",
    "end\n",
    "\n",
    "#=\n",
    "struct Chain\n",
    "    layers; lambda1; lambda2; loss; deeperCutOption\n",
    "    Chain(layers...; lambda1=0, lambda2=0, loss=nll, deeperCutOption=nothing) = new(layers, lambda1, lambda2, loss, deeperCutOption)\n",
    "end\n",
    "\n",
    "# The prediction and average loss do not change\n",
    "function (c::Chain)(x)  \n",
    "    connection_from3_to5 = nothing;\n",
    "    for l in c.layers    \n",
    "         x = l(x)\n",
    "        \n",
    "        if c.deeperCutOption.connect_res3_to_res5 \n",
    "            layer_tag = get_object_tag(l)\n",
    "            if layer_tag == 3 \n",
    "                connection_from3_to5 = l.conv3_for_deepercut_output(x)\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        if c.deeperCutOption.connect_res3_to_res5 \n",
    "            layer_tag = get_object_tag(l)\n",
    "            if layer_tag == \"part_detect_deconv\" \n",
    "                x = x .+ connection_from3_to5 \n",
    "            end\n",
    "        end\n",
    "         \n",
    "    end \n",
    "    \n",
    "    x\n",
    "end\n",
    "\n",
    "function (c::Chain)(x, y)\n",
    "    loss = c.loss(c(x), y)\n",
    "    if training() # Only apply regularization during training, only to weights, not biases.\n",
    "        c.lambda1 != 0 && (loss += c.lambda1 * sum(sum(abs, l.w) for l in c.layers))\n",
    "        c.lambda2 != 0 && (loss += c.lambda2 * sum(sum(abs2, l.w) for l in c.layers))\n",
    "    end\n",
    "    return loss\n",
    "end\n",
    "(c::Chain)(d::Data) = mean(c(x, y) for (x, y) in d)\n",
    "=#\n",
    "\n",
    "#=\n",
    "struct Deconv; w; stride; padding; tag::String; end\n",
    "Deconv(w1, w2, nx, ny; stride=1, padding=0, atype=Knet.atype(), tag=\"\") = Deconv(param(w1, w2, nx, ny;atype=atype), stride, padding, tag)\n",
    "\n",
    "(dc::Deconv)(x) = deconv4(dc.w, x; stride=dc.stride, padding=dc.padding)\n",
    "=#\n",
    "\n",
    "# This became redundant now\n",
    "struct ResLayerConv; w; padding; stride; end\n",
    "# Random init\n",
    "ResLayerConv(w1, w2, nx, ny; padding=0, stride=1) = ResLayerConv(param(w1, w2, nx, ny), padding, stride)\n",
    "# Predeterminde weights\n",
    "ResLayerConv(w; padding=0, stride=1) = ResLayerConv(param(w), padding, stride)\n",
    "(rl0::ResLayerConv)(x) = conv4(rl0.w, x; padding=rl0.padding, stride=rl0.stride)\n",
    "\n",
    "struct BatchNormLayer; w; ms; \n",
    "\n",
    "    function BatchNormLayer(pre_w, pre_ms)\n",
    "        res_mean = popfirst!(pre_ms)\n",
    "        # Trick to arrange variance value for new(er) batchnorm\n",
    "        res_variance =   popfirst!(pre_ms).^2  .- 1e-5\n",
    "        ms = bnmoments(mean=res_mean, var=res_variance)\n",
    "    \n",
    "        w1 = pre_w[1]\n",
    "        w2 = pre_w[2]\n",
    "        w1 = vec(w1)\n",
    "        w2 = vec(w2)\n",
    "        w =  vcat(w1, w2)\n",
    "        param_w = param(w, atype=Knet.atype())\n",
    "        return new(param_w, ms)\n",
    "    end\n",
    "\n",
    "end\n",
    "\n",
    "function (batch_norm_layer::BatchNormLayer)(x)\n",
    "    return batchnorm(x, batch_norm_layer.ms, batch_norm_layer.w; eps=1e-5)\n",
    "end \n",
    "\n",
    "# ResNet 50 initial layer\n",
    "struct ResLayerX1_50;\n",
    "    batch_layer;\n",
    "    conv_w;\n",
    "    conv_b;\n",
    "    padding;\n",
    "    stride;\n",
    "    pool_window_size;\n",
    "    pool_stride;\n",
    "    pool_padding;\n",
    "end\n",
    "\n",
    "function ResLayerX1_50(w, ms; padding=3, stride=2, pool_window_size=3, pool_stride=2, pool_padding=1)\n",
    "    bnl = BatchNormLayer(w[3:4], ms)\n",
    "    return ResLayerX1_50(bnl,\n",
    "        param(w[1]; atype=Knet.atype()),\n",
    "        param(w[2]; atype=Knet.atype()),\n",
    "        padding,\n",
    "        stride,\n",
    "        pool_window_size,\n",
    "        pool_stride,\n",
    "    pool_padding)\n",
    "end\n",
    "\n",
    "function (rlx1_50::ResLayerX1_50)(x)\n",
    "    o = conv4(rlx1_50.conv_w, x; padding=rlx1_50.padding, stride=rlx1_50.stride) .+ rlx1_50.conv_b\n",
    "    o = rlx1_50.batch_layer(o)\n",
    "    o = relu.(o)\n",
    "    o = pool(o; window=rlx1_50.pool_window_size, stride=rlx1_50.pool_stride, padding=rlx1_50.pool_padding)\n",
    "    return o\n",
    "end\n",
    "\n",
    "# X0\n",
    "struct ResLayerX0; batch_layer; conv_w; padding; stride; dilation; end\n",
    "# Predetermined weights\n",
    "# TODO: should we try to make bnl params??\n",
    "function ResLayerX0(w, ms; padding=0, stride=1, dilation=1)\n",
    "    bnl = BatchNormLayer(w[2:3], ms)\n",
    "    return   ResLayerX0(\n",
    "        bnl,        \n",
    "        param(w[1]; atype=Knet.atype()), \n",
    "        padding, \n",
    "        stride,\n",
    "        dilation\n",
    "    )\n",
    "end\n",
    "\n",
    "function (rlx0::ResLayerX0)(x) \n",
    "   # batchnorm_as_function(rlx0.batch_w, conv4(rlx0.conv_w, x; padding=rlx0.padding, stride=rlx0.stride), rlx0.ms) \n",
    "    o = conv4(rlx0.conv_w, x; padding=rlx0.padding, stride=rlx0.stride, dilation=rlx0.dilation)\n",
    "    o = rlx0.batch_layer(o)\n",
    "    return o\n",
    "end\n",
    "\n",
    "# X1\n",
    "struct ResLayerX1; x0_layer; is_initial::Bool; end\n",
    "ResLayerX1(w, ms; padding=0, stride=1, is_initial::Bool=false, dilation=1) = ResLayerX1(ResLayerX0(w, ms; padding=padding, stride=stride, dilation=dilation), is_initial)\n",
    "function (rlx1::ResLayerX1)(x)\n",
    "    relu_res = relu.(rlx1.x0_layer(x))\n",
    "    if rlx1.is_initial\n",
    "        return pool(relu_res; window=3, stride=2)\n",
    "    else\n",
    "        return relu_res\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "# X2\n",
    "# TODO: can be constructed like Chain\n",
    "struct ResLayerX2; x1_a_layer; x1_b_layer; x0_c_layer; end\n",
    "ResLayerX2(w, ms; pads=[0, 1, 0], strides=[1, 1, 1], dilations=[1,1,1]) = ResLayerX2(\n",
    "    ResLayerX1(w[1:3], ms; padding=pads[1], stride=strides[1], dilation=dilations[1]),\n",
    "    ResLayerX1(w[4:6], ms; padding=pads[2], stride=strides[2], dilation=dilations[2]),\n",
    "    ResLayerX0(w[7:9], ms; padding=pads[3], stride=strides[3], dilation=dilations[3])\n",
    "    )\n",
    "(rlx2::ResLayerX2)(x) =   rlx2.x0_c_layer(rlx2.x1_b_layer((rlx2.x1_a_layer(x))))\n",
    "\n",
    "\n",
    "# X3\n",
    "struct ResLayerX3; x0_a_layer::ResLayerX0; x2_b_layer::ResLayerX2; end\n",
    "ResLayerX3(w, ms; pads=[0, 0, 1, 0], strides=[2, 2, 1, 1], b_layer_dilations=[1, 1, 1]) = ResLayerX3(\n",
    "    ResLayerX0(w[1:3], ms; padding=pads[1], stride=strides[1]),\n",
    "    ResLayerX2(w[4:12], ms; pads=pads[2:4], strides=strides[2:4], dilations=b_layer_dilations)\n",
    "    )\n",
    "function (rlx3::ResLayerX3)(x)\n",
    "    res_a = rlx3.x0_a_layer(x)\n",
    "    res_b = rlx3.x2_b_layer(x)\n",
    "    return relu.(res_a .+ res_b)\n",
    "end\n",
    "\n",
    "\n",
    "# X4\n",
    "struct ResLayerX4;  x2_layer; end\n",
    "ResLayerX4(w, ms; pads=[ 0, 1, 0], strides=[1, 1, 1], dilations=[1,1,1]) = ResLayerX4(\n",
    "    ResLayerX2(w, ms; pads=pads, strides=strides, dilations=dilations)\n",
    "    )\n",
    "(rlx4::ResLayerX4)(x) =   relu.(x .+ rlx4.x2_layer(x))\n",
    "\n",
    "\n",
    "# X5\n",
    "#=\n",
    "struct ResLayerX5;\n",
    "    x3_layer::ResLayerX3;\n",
    "    x4_layers; \n",
    "    is_next_fc::Bool;\n",
    "    conv3_for_deepercut_output;\n",
    "    tag;\n",
    "end\n",
    "\n",
    "\n",
    "function ResLayerX5(w, ms; \n",
    "        strides=[2, 2, 1, 1],\n",
    "        is_next_fc::Bool=false, \n",
    "        b_layer_dilations=[1, 1, 1],\n",
    "        b_layer_pads=[0, 1, 0],\n",
    "        is_conv3_for_deepercut=false,\n",
    "        tag=nothing\n",
    "    )\n",
    "    x3_layer::ResLayerX3 = ResLayerX3(w[1:12], ms; strides=strides)\n",
    "    x4_layers = []\n",
    "    for k = 13:9:length(w)\n",
    "        layer = ResLayerX4(w[k:k + 8], ms; dilations=b_layer_dilations, pads=b_layer_pads)\n",
    "        push!(x4_layers, layer)\n",
    "    end\n",
    "    \n",
    "    conv3_for_deepercut_output = nothing\n",
    "    if is_conv3_for_deepercut\n",
    "            # 14 layer for part detection\n",
    "        conv3_for_deepercut_output = Conv2(1, 1, 512, global_num_joints, identity; is_pool_enabled=false)\n",
    "    end\n",
    "    \n",
    "    return ResLayerX5(x3_layer, x4_layers, is_next_fc, conv3_for_deepercut_output, tag)\n",
    "    end \n",
    "=#\n",
    "\n",
    "#=\n",
    "function (rlx5::ResLayerX5)(x) \n",
    "    x = rlx5.x3_layer(x)\n",
    "    for l in rlx5.x4_layers\n",
    "        x = l(x)\n",
    "    end\n",
    "\n",
    "    if rlx5.is_next_fc\n",
    "        return pool(x; stride=1, window=7, mode=2)\n",
    "    else\n",
    "        return x\n",
    "    end\n",
    "    end \n",
    "=#\n",
    "\n",
    "# OLD IMPLEMENTATIONS's \n",
    "\n",
    "struct ResLayerX5;\n",
    "    x3_layer::ResLayerX3;\n",
    "    x4_layers; \n",
    "    is_next_fc::Bool;\n",
    "end\n",
    "\n",
    "function (rlx5::ResLayerX5)(x) \n",
    "    x = rlx5.x3_layer(x)\n",
    "    for l in rlx5.x4_layers\n",
    "        x = l(x)\n",
    "    end\n",
    "\n",
    "    if rlx5.is_next_fc\n",
    "        return pool(x; stride=1, window=7, mode=2)\n",
    "    else\n",
    "        return x\n",
    "    end\n",
    "    end \n",
    "\n",
    "function ResLayerX5(w, ms; \n",
    "        strides=[2, 2, 1, 1],\n",
    "        is_next_fc::Bool=false, \n",
    "        b_layer_dilations=[1, 1, 1],\n",
    "        b_layer_pads=[0, 1, 0]\n",
    "    )\n",
    "    x3_layer::ResLayerX3 = ResLayerX3(w[1:12], ms; strides=strides)\n",
    "    x4_layers = []\n",
    "    for k = 13:9:length(w)\n",
    "        layer = ResLayerX4(w[k:k + 8], ms; dilations=b_layer_dilations, pads=b_layer_pads)\n",
    "        push!(x4_layers, layer)\n",
    "    end\n",
    "    return ResLayerX5(x3_layer, x4_layers, is_next_fc)\n",
    "end \n",
    "\n",
    "struct Deconv; w; stride; padding; end\n",
    "Deconv(w1, w2, nx, ny; stride=1, padding=0, atype=Knet.atype()) = Deconv(param(w1, w2, nx, ny;atype=atype), stride, padding)\n",
    "(dc::Deconv)(x) = deconv4(dc.w, x; stride=dc.stride, padding=dc.padding)\n",
    "\n",
    "struct Chain\n",
    "    layers; lambda1; lambda2; loss;\n",
    "    Chain(layers...; lambda1=0, lambda2=0, loss=nll) = new(layers, lambda1, lambda2, loss)\n",
    "end\n",
    "\n",
    "# The prediction and average loss do not change\n",
    "function (c::Chain)(x)  \n",
    "    for l in c.layers    \n",
    "         x = l(x)     \n",
    "    end \n",
    "    x\n",
    "end\n",
    "\n",
    "function (c::Chain)(x, y)\n",
    "    loss = c.loss(c(x), y)\n",
    "    if training() # Only apply regularization during training, only to weights, not biases.\n",
    "        c.lambda1 != 0 && (loss += c.lambda1 * sum(sum(abs, l.w) for l in c.layers))\n",
    "        c.lambda2 != 0 && (loss += c.lambda2 * sum(sum(abs2, l.w) for l in c.layers))\n",
    "    end\n",
    "    return loss\n",
    "end\n",
    "(c::Chain)(d::Data) = mean(c(x, y) for (x, y) in d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compute_error_in_training (generic function with 1 method)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"../training.jl\")\n",
    "include(\"../plots.jl\")\n",
    "include(\"../helper.jl\")\n",
    "include(\"accuracy.jl\")\n",
    "include(\"loss.jl\")\n",
    "include(\"../training.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_batches = Knet.load(\"20-12-2020-24kimage-batches.jld2\", \"24k-image-batches\");\n",
    "dtrn, dval = loaded_batches;\n",
    "dtrn_x1, dtrn_y1 = first(dtrn)\n",
    "dval_x1, dval_y1 = first(dval)\n",
    "resnet_with_deconv = Knet.load(\"20-+5ep-12-2020-24kimage-resnet-50.jld2\", \"resnet_with_deconv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_scs_gt = []\n",
    "wrong_scs_pred = []\n",
    "wrong_scs_pred_initial = []\n",
    "wrong_scs_cache= [wrong_scs_gt, wrong_scs_pred, wrong_scs_pred_initial];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4097842261904763"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelized_naive_pck_sigm_with_wrong_cache(resnet_with_deconv, dval, wrong_scs_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_scs_gt;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_idx = 136"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 43008 Total number of Joints\n",
    "test_y = wrong_scs_gt[ wrong_idx ];\n",
    "test_y_pred = wrong_scs_pred[wrong_idx];\n",
    "test_y_pred_initial = wrong_scs_pred_initial[wrong_idx];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#findall(x -> x[12, 12] == 1  , wrong_scs_gt )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CartesianIndex(9, 17)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.59941936f0"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max = argmax(test_y_pred_initial)\n",
    "println(max)\n",
    "test_y_pred_initial[max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15-element Array{CartesianIndex{2},1}:\n",
       " CartesianIndex(9, 10)\n",
       " CartesianIndex(10, 10)\n",
       " CartesianIndex(11, 10)\n",
       " CartesianIndex(8, 11)\n",
       " CartesianIndex(9, 11)\n",
       " CartesianIndex(10, 11)\n",
       " CartesianIndex(11, 11)\n",
       " CartesianIndex(8, 12)\n",
       " CartesianIndex(9, 12)\n",
       " CartesianIndex(10, 12)\n",
       " CartesianIndex(11, 12)\n",
       " CartesianIndex(12, 12)\n",
       " CartesianIndex(9, 13)\n",
       " CartesianIndex(10, 13)\n",
       " CartesianIndex(11, 13)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findall(x->x==1, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CartesianIndex(9, 10)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_gt = argmax(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0f0"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y[max_gt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tbody><tr><td style='text-align:center;vertical-align:middle; margin: 0.5em;border:1px #90999f solid;border-collapse:collapse'><img style='max-width: 100px; max-height:100px;display:inline' src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAIAAAACAAQAAAADrRVxmAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QAAd2KE6QAAAAmSURBVEjHY2AYBcMb/KdcgP//h2EjwMDPQLnAKBgFo2AUjAJKAQBjehwhId6+7AAAAABJRU5ErkJg\"></td><td style='text-align:center;vertical-align:middle; margin: 0.5em;border:1px #90999f solid;border-collapse:collapse'><img style='max-width: 100px; max-height:100px;display:inline' src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAAAAADmVT4XAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAACKSURBVHja7dShjUJBAEXRy6KQJCvBIqgCiV+zgnqohB5AUgeCBtBIEgwN8FeQnz0nGTmTl5eXKQAAAP67yV8ub6tTtanOA9/4+nQDAgzawK5aVdPquzpW62o/xgYEGLSBn+peLatr9cg/MOIAb2/gt5q9zq2aV5dqUR3G2IAAAAAAAAAAAAAAAPAEnx4LHYTD6g8AAAAASUVORK5C\"></td><td style='text-align:center;vertical-align:middle; margin: 0.5em;border:1px #90999f solid;border-collapse:collapse'><img style='max-width: 100px; max-height:100px;display:inline' src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAAAAADmVT4XAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAHdSURBVHja7ZpLT8JQEIW/toiCGokYCQvd+Fi4ceN/d+UfcWGicWF8RiM+0FoXM6QVCkHF9CaeLyGXyy1wmDMztxRACCHEfyeaxZOzX7xGXHUEJKA2zUER5vPwiI8x8DFmPfgISMBUfSAaGuPCvPgJUsz74i34CEhANGkh8fsJud8xsEju7xJwBawA10ADeADmgFc/ZlIuVB4BCSjdCwaqmpiPy0Af8zcFupivXeAZ2ARefL0OvAHrwDmWP88hR0ACSvtA7Aurfn/Z523M+x1gF/N3DTgE9oAjX7/BcifDesITdr6QjXmvSpGA0hxIXFnH5yvAI1b318A+0AM2gBOsN7xgdd8CzrDcuPDnXqIcCFjAl72gmBAxeR2vkZ8bHGA9v4OdB2wBx8A2cIrlSgzc+uv1Qo+ABIz0gRjzu+3zms8TrL+3sP0+8jHF9vsacAcsAPc+f/exz/jvi5VHQAK+5MBATR2Yx/zLsD5ex2p6AfO8idV8A9sHIj/u3UcY9Vw5EKSA0j4A5nWfvI4TrO4He0RZXX/3+lAQEZCAkT4wuO6XkNdzSl7nP7kWGHQEJCAqeyDD6j/1x2bpeXARkAAhhBBCCCGEEEIIIYQQQgghhBAi+qv/BUxL5b+eS8AnNGpi33mVsAEAAAAASUVORK5C\"></td></tr></tbody></table><div><small>(a vector displayed as a row to save space)</small></div>"
      ],
      "text/plain": [
       "3-element Array{Base.ReinterpretArray{Gray{Float32},2,Float32,Array{Float32,2}},1}:\n",
       " [Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0); Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0); … ; Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0); Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0)]\n",
       " [Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0); Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0); … ; Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0); Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0)]\n",
       " [Gray{Float32}(0.0016089114f0) Gray{Float32}(0.0001859641f0) … Gray{Float32}(2.2433736f-5) Gray{Float32}(0.0006538356f0); Gray{Float32}(0.00015891755f0) Gray{Float32}(6.6565026f-6) … Gray{Float32}(1.5673205f-6) Gray{Float32}(2.8038414f-5); … ; Gray{Float32}(5.729311f-5) Gray{Float32}(1.452095f-5) … Gray{Float32}(2.4183832f-6) Gray{Float32}(9.524267f-5); Gray{Float32}(0.004686711f0) Gray{Float32}(0.00021577324f0) … Gray{Float32}(3.709285f-5) Gray{Float32}(0.00065490726f0)]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_be_shown = [colorview(Gray, test_y),\n",
    "colorview(Gray, test_y_pred),\n",
    "colorview(Gray, test_y_pred_initial)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
